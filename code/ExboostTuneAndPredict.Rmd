---
title: "Digital soil mapping"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo = FALSE}
library(caret)
library(caretEnsemble)
library(doParallel) 
library(cluster)
library(earth)
library(xgboost)
library(mlbench)
library(pROC)
library(plyr)
library(dplyr)
library(corrplot)
library(ggplot2)
library(ggthemes)
```

#Load training data and raster data
```{r}
OriginalDataNumeric<-read.table("C:/data/lokala_exe/TrainingData/TrainingData.txt",  stringsAsFactors=TRUE, header = TRUE, dec = ",") 
OriginalDataNumeric<-na.omit(OriginalDataNumeric)
OriginalDataNumeric$FID <-NULL
OriginalDataNumeric$soil<-NULL
OriginalDataNumeric$POINT_X<-NULL
OriginalDataNumeric$POINT_Y<-NULL
OriginalDataNumeric$Process<-NULL
OriginalData<-OriginalDataNumeric


colsF<-c("Proesskod", "NMD")
OriginalData[colsF]<-lapply(OriginalData[colsF], factor)
str(OriginalData)
```
```{r}
# Data distribution
library(ggplot2)
cols <- c("1"="red","2"="lightblue", "3"="blue", "4"="grey", "5"="yellow", "6"="green", "7"="brown", "8"="yellow", "9"="orange")

g<-ggplot(data=OriginalData, aes(x=Proesskod)) + 
  geom_bar(aes(fill = Proesskod)) +
  labs(title = "", x = "", y = "Number of points")

g + theme_bw() + theme(legend.position = "none") + theme(axis.text = element_text(angle = 90),panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = "black")) + scale_fill_manual(values = cols)
```
The features are better described in the paper mentioned in the beginning but here is a short translation:  
**Proesskod** = Quartenary deposits 
Berg = 1
Morän = 3
Glaciala finsediment = 5
Glacifluviala sediment = 6
Torv = 7
Postglaciala finsediment = 8
Postglaciala grovsediment = 9

Extreme gradient boosting is resistant to highly correlated predictors, but multicollinearity may
negatively affect interpretability of the model. First we will plot and inspect the correlations.

```{r}
set.seed(42)
#Remove label
tempMCL<-OriginalDataNumeric[,-9] #the label is the 9th column.

#create and plot correlation table
tempCOR <- cor(tempMCL) 
corrplot(tempCOR, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 55, tl.cex = .6)
```




Split data into training and testing data. a common practice is to use 70- 80% of the data as training data and the remaining 20-30 % as testing data. In this case 70 % of the data will be used as training data while 30 % will be used for testing the trained model.

```{r}
set.seed(42)

#split original data for training final model
TrainingDataIndex<- createDataPartition(OriginalData$Proesskod, p=0.7, list = FALSE) #0.7 means that 70 % of the data will be used as training data.
training <- OriginalData[ TrainingDataIndex,]
testing <- OriginalData[-TrainingDataIndex,]
```

## Set up and tune XGBOOST

The learner used is "Extreme gradient boosting with dropout"  or [xgbDART](https://xgboost.readthedocs.io/en/latest/index.html). 

There are a number of settings we can adjust in this model. These settings are refered to as paremters or hyperparameters in machine learning lingo and the best values for these parameters will vary with each dataset. We will therefore use some brute force and try combinations of many parameters using a [tune grid](http://topepo.github.io/caret/model-training-and-tuning.html). This aproach will train many models with different parameters to see what works best. This is a very time consuming process. 

Bellow is a short description of each paramter taken from the xgboost documentation:
<br/>
**nrounds:** number of decision trees to build.

**max_depth:** Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. 0 is only accepted in lossguided growing policy when tree_method is set as hist and it indicates no limit on
<br/>

**depth:** Beware that XGBoost aggressively consumes memory when training a deep tree.

**eta:** Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.

**gamma:** Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be

**colsample:** This is a family of parameters for subsampling of columns. bytree is the subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.

**min_child_weigth:** Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression task, this simply corresponds to minimum number of instances needed to be in each node. The larger min_child_weight is, the more conservative the algorithm will be.

**drop_rate:** Dropout rate (a fraction of previous trees to drop during the dropout).

**skip_drop:** Probability of skipping the dropout procedure during a boosting iteration.

**subsample:** Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration.

500 trees = kappa 0.7488, accuracy = 0.8099 
1000 trees = kappa 0.7485, accuracy = 0.8097

```{r}
set.seed(42)

# tune control is the way the model is internaly tested during tuning. "cv" means cross validation and number = 5 means that we will use 5 fold cross validation. Allow parallel means that we will use parallel procesing to speed up the process. 
tune_control <- caret::trainControl(method = "cv", number = 5, allowParallel = TRUE, verboseIter = TRUE)

#Tune grid is used to test multiple different hyperparameters and test different models at the same time. 
gridTuning = expand.grid(
  nrounds = c(10,50,100,250,500), 
  max_depth = c(4,6,8,10), 
  eta = c(0.3), 
  gamma = c(0), 
  colsample_bytree = c(1), 
  min_child_weight = c(1),
  rate_drop = c(0),
  skip_drop = c(0),
  subsample = c(1)
)

#This is where we set upp the tuning model using the training data and hyperparameters in the tune grid above.
xgbDARTTuneModel <- caret::train(Proesskod~., data = training, trControl = tune_control, tuneGrid = gridTuning, method = "xgbDART", verbose = FALSE, metric="Kappa")
registerDoSEQ()
```

Inspect result from tuning
```{r}
#plot result from tune grid to see which hyperparameters that result in optimal preformance.
tuninggrid<-ggplot(xgbDARTTuneModel, metric = "Kappa", plotType = "scatter",
     scales = list(x = list(rot = 90))) + theme_bw() + theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = "black")) 
plot(tuninggrid)
```

Note that the distance between the lines is smaller with higher tree depth and the number of boosting iterations don't seem to have a large impact after the first 100 trees.

We can also print the trained model for more details
```{r}
print(xgbDARTTuneModel) #Note that the best combinations of hyperparameters is listed at the bottom.
```
Kappa was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 1000, max_depth = 10, eta = 0.3, gamma = 0, subsample = 1, colsample_bytree = 1, rate_drop = 0, skip_drop = 0 and min_child_weight = 1.


Now it's time to test the model on the test data. Remember that we did not train on this data. The metric we are interested in is the Kappa value: https://en.wikipedia.org/wiki/Cohen%27s_kappa

Kappa values < 0 can be interpeted as indicating no agreement and 0-0.20 as slight, 0.21-0.40 as fair, 0.41-0.60 as moderate, 0.61-0.80 as substantial, and 0.81-1 as almost perfect agreement: Landis, J.R.; Koch, G.G. (1977). "The measurement of observer agreement for categorical data". Biometrics. 33 (1): 159-174. doi:10.2307/2529310.

```{r}
#test model
caret::confusionMatrix(
  data = predict(xgbDARTTuneModel, testing),
  reference = testing$Proesskod
  )
```
The model have a kappa value of 0.83 and overall accuracy of 87 %.

Some features are more important than others and the least important ones can potentailly be removed in order to save time and storage space. 

```{r}
#Variable importance. This plot will find and plot the 20 most important variables.
ImportantVariables<-varImp(xgbDARTTuneModel, scale = FALSE)
ggplot(ImportantVariables, top = 20) +
  theme_bw() + theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"))
```


When you have decided on which features to use and tuned the model with grid tunning it's time to set up the final model.
This model will use the hyperparameters from the grid tuning but we will use all avalible data to train the model.
When repporting the accuracy of the model you can refer back to the accuracy obtained when the model was trained on the training data and tested on the test data.

```{r}
set.seed(42)

control <- caret::trainControl(method = "none", verboseIter = TRUE)

#The vales in the tune grid was seleted with a grid search aproach on the trainingdataset. These are the best values.
FinalParameters = expand.grid(
  nrounds = c(500), 
  max_depth = c(10), 
  eta = c(0.3), 
  gamma = c(0), 
  colsample_bytree = c(1), 
  min_child_weight = c(1),
  rate_drop = c(0),
  skip_drop = c(0),
  subsample = c(1)
)

#Train model using all data. This will take a while.
xgbDARTFinalmodel <- caret::train(Proesskod~., data = OriginalData, trControl = control, tuneGrid = FinalParameters, method = "xgbDART", verbose = TRUE, metric="Kappa")
```

## Implement the final model in production

Now it's time to put the model into production and predict quartenary deposits on multiband rasters instead of a dataframe. The prediction will be done in parallel where each thread will be used to predict a separate raster. The multiband rasters was created using the arcgis pro tool "Composite bands". see pythonscripts "Master_multithread_CompositeBands.py and Slave_CompositeBands.py" 

```{r echo=TRUE, warning=FALSE, error=FALSE, message=FALSE}
library(doParallel)
library(cluster)
#Define how many cores you want to use. R will detect how many threads your machine have. One thread is commonly left out to handle background processes such as the operating system hence -1.
UseCores <- detectCores() -1

#Register Cluster
cl  <- makeCluster(UseCores)
registerDoParallel(cl)

#Path to folder where all multiband rasters are saved. keep in mind that the output is saved in the same folder as the input. Remove them before running the script again.

InputPath <- "D:/WilliamLidberg/FeatureExtraction/CompositeBands" #All of Sweden. Use this path when predicting all of Sweden.
OutputPath <- "D:/WilliamLidberg/FeatureExtraction/Prediction"

#InputPath <- "D:/WilliamLidberg/CustomPrediction/DemoAreasCompositeRasters" #Demo areas
#OutputPath <- "D:/WilliamLidberg/CustomPrediction/DemoAreasPrediction"


stack_list <- list.files(InputPath, pattern=".tif$", full.names=T) #File ending is case sensetive ".TIF" and ".tif" is not the same.

ptm <- proc.time()
#Use foreach loop and %dopar% command
foreach(i=1:length(stack_list)) %dopar% {
  library(raster)
  library(caret)

  img  <- stack(stack_list[i])
  #Set names of each band in the multiband raster. It's important that these are the same as in the training data.
  #Note that the order of bands sould b the same as the raster composites. see pythonscripts.
  names(img)[1]<-paste("DEM") 
  names(img)[2]<-paste("EAS1ha")
  names(img)[3]<-paste("EAS10ha")
  names(img)[4]<-paste("DI2m")
  names(img)[5]<-paste("CVA")
  names(img)[6]<-paste("SDFS")
  names(img)[7]<-paste("DFME")
  names(img)[8]<-paste("Rugged")
  names(img)[9]<-paste("NMD")
  names(img)[10]<-paste("soil")
  names(img)[11]<-paste("HKDepth")
  names(img)[12]<-paste("SoilDepth")
  names(img)[13]<-paste("LandAge")
  names(img)[14]<-paste("MSRM")
  names(img)[15]<-paste("POINT_X")
  names(img)[16]<-paste("POINT_Y")
  names(img)[17]<-paste("MED")


  
  #OutputName
  outnamesoil <- sub(pattern     = InputPath,
                 replacement = OutputPath, 
                 x           = stack_list[i])
  
  #Predict
  map  <- predict(img, xgbDARTFinalmodel, factorlist=list(NMD=levels(training$NMD)), na.rm=TRUE, overwrite = TRUE)
  
  #Save prediction to raster
  writeRaster(map, filename  = outnamesoil, overwrite = TRUE)

 
}

#end cluster. make sure to run these two rows even when the script fails to make sure that the cluster is shut down.
stopCluster(cl)
registerDoSEQ()
proc.time() - ptm
```


